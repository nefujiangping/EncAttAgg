# data for train/test
data_path: "./data/DocRED"
bert_embedding_dir: "./data/DocRED"
checkpoint_dir: "./checkpoint"
log_dir: "./log"
summary_dir: "./summary"

dataset: "DocRED"
train_prefix: "dev_train"  # dev_trainXXXX will be loaded for training
test_prefix: "dev_dev"

# embedding
max_length: 512
use_bert_embedding: true
bert_embedd_dim: 768
# if use_bert_embedding=true and embedd_dim < bert_embedd_dim,
#  bert embedding will be down-project to bert_embedd_dim
embedd_dim: 100
# For `use_bert_embedding`=false, whether to freeze the embedding matrix
freeze_embedd: true
drop_word:
  use_drop: false  # For DocRED, we do not use drop word, because of bert embedding
  drop: 0.5  # ratio of drop word
entity_type_size: 20
entity_type_num: 7
coref_size: 20
dis_size: 20

# encoder
nlayer: 2  # number of BiLSTM layer
cat_nlayer: false  # Whether cat output of BiLSTM layers
lstm_keep_prob: 0.8
hidden_size: 128

# attenders
entity_span_pooling: "mean"  # average all tokens of a mention to get the mention representation
coref_pooling: "mean"  # average all mentions of an entity to get the entity representation
which_model: "BiLSTM-M"  # EncAgg, BiLSTM-M, BRAN-M
mutual_attender:
  attender: "NONE"  # ML-MMA(MultiLayer Mutual Multi-Head Attention), NONE
  num_layers: 1
  nhead: 8
  shared: true
  drop: 0.1
integration_attender:
  attender: "NONE"  # ML-MA(MultiLayer Multi-Head Attention), NONE
  num_layers: 1
  nhead: 8
  drop: 0.1
use_distance: true
use_overlap: false
use_bilinear: true  # Using Bilinear performs better for BiLSTM-M model
relation_num: 97  # 96 relation types plus `NONE`

# other hyper-parameters
batch_size: 32
init_lr: 1.0e-3  # 1e-3
use_lr_scheduler: false
train_rel_limit_per_example: 1800  # 200 (the same setting as EncAttAgg model) performs worse
max_epoch: 50
epoch_start_to_eval: 25
# For test
test_batch_size: 32
test_rel_limit_per_example: 1800
# filter out those entity pairs whose head-tail entity type combination does not appear in the pos pairs of training and dev sets.
filter_by_entity_type: false




