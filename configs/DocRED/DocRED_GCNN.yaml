# data for train/test
data_path: "/home/jp/workspace2/DocRED/code/prepro_data"
bert_embedding_dir: "/home2/public/jp/DocRED"
checkpoint_dir: "./checkpoint"
log_dir: "./log"
summary_dir: "./summary"  # this dir saves the training logs (use tensorboard to visualize); backup codes

dataset: "DocRED"
train_prefix: "dev_train"  # dev_trainXXXX will be loaded for training
test_prefix: "dev_dev"

# embedding
max_length: 512 # max length of CDR is 620
use_bert_embedding: true
bert_embedd_dim: 768
embedd_dim: 100
# For `use_bert_embedding`=false, whether to freeze the embedding matrix
freeze_embedd: true # tune the PubMed embedding when training
entity_type_num: 7  # 2 entity types (Chemical, Disease) plus `NONE`
position_dim: 20  # gcn_in_dim: 2*position_dim + embedd_dim
inp_dropout: 0.1

# GCN encoder
gcn_dim: 140
num_edge_types: 10
num_unRare_edge_types: 4
num_blocks: 2
gcn_use_gate: true
gcn_dropout: 0.05
gcn_residual: true  # if `gcn_residual` is set to be true, make sure that `gcn_dim=gcn_in_dim`
bi_affine_ff_dim: 140
bi_affine_dropout: 0.05

# attenders
entity_span_pooling: "mean"  #
which_model: "GCNN"  # EncAgg, BiLSTM, BRAN-M
use_bilinear: true  # This is useless for GCNN, GCNN always use bilinear
relation_num: 97  # For DocRED, 96 relation types plus `NONE`

# other hyper-parameters
batch_size: 100
train_rel_limit_per_example: 200  # use part of negative pairs, to train
# #Pos and #Negative sample ratio of DocRED is 3.8w : 116w
# #pos:#neg == 1:neg_sample_multiplier, e.g. 1:1, 1:3, 1:5, ...
# arg1 = train_rel_limit_per_example
# arg2 = #pos + #pos*neg_sample_multiplier
# arg3 = #entity pairs of the document
# final training #entity-pairs = min(arg1, arg2, arg3) for each document
use_neg_sample: false
neg_sample_multiplier: 10
# filter out those entity pairs whose head-tail entity type combination does not appear in the pos pairs of training and dev sets.
filter_by_entity_type: false
init_lr: 5.0e-4  # 5.0e-4
lr_decay: 0.95  # lr_decay=1. means no decay
use_lr_scheduler: false
#grad_clip_value: 10
max_epoch: 15
epoch_start_to_eval: 2

# For test
test_batch_size: 64
test_rel_limit_per_example: 1800  # all possible pairs should be fed for prediction

# average the parameters over the recent epochs, to to make the test performance be stable
avg_params:
  use_avg_params: False
  num_recent_epochs: 10  # `0` means averaging the parameters up to this epoch
parallel_forward: true




