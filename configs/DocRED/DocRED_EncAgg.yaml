# data for train/test
data_dir: "./data/prepro_data/DocRED"
checkpoint_dir: "./checkpoint"
log_dir: "./log"
summary_dir: "./summary"

dataset: "DocRED"
train_prefix: "dev_train"
test_prefix: "dev_dev"


# model
model_name: 'bert-base-cased'
transformer_type: 'bert'
freeze_embed: true
embedd_dim: 768
word_dropout: 0.0  # <= 0.0 means no dropout
max_length: 512
use_entity_cluster: true
coref_size: 20
use_entity_type: true
entity_type_num: 7
entity_type_size: 20

hidden_size: 128
num_bilstm_layers: 2
lstm_keep_prob: 0.8

entity_span_pooling: 'mean'  # See constant.PoolingStyle

dis_size: 20
use_distance: true
mutual_attender:
  attender: "NONE"  # ML_MMA (MultiLayer Mutual Multi-Head Attention), NONE; See constant.MutualAttender
  num_layers: 1
  nhead: 8
  shared: true
  drop: 0.1
integration_attender:
  attender: "NONE"  # ML_MA (MultiLayer Multi-Head Attention), NONE; See constant.IntegrationAttender
  num_layers: 1
  nhead: 8
  drop: 0.1
use_overlap: true
# logging:
relation_num: 97

# training
train_h_t_limit: 200
use_neg_sample: false
neg_sample_multiplier: 5
batch_size: 4
test_batch_size: 1
learning_rate: 2.0e-4
bert_learning_rate: 2.0e-5
optimizer: Adam
max_epoch: 50
epoch_start_to_eval: 25
accumulation_steps: 1
max_grad_norm: 0.0
evaluation_steps: 0  # if > 0, every `evaluation_steps` evaluate model on dev-set
train_log_steps: 50
