# data for train/test
data_path: "./data/DocRED"
bert_embedding_dir: "./data/DocRED"
checkpoint_dir: "./checkpoint"
log_dir: "./log"
summary_dir: "./summary"

dataset: "DocRED"
train_prefix: "dev_train"  # dev_trainXXXX will be loaded for training
test_prefix: "dev_dev"

# embedding
max_length: 512 # max length of DocRED is 512
use_bert_embedding: true
bert_embedd_dim: 768
# if use_bert_embedding=true and embedd_dim < bert_embedd_dim,
#  bert embedding will be down-project to bert_embedd_dim
embedd_dim: 100
# For `use_bert_embedding`=false, whether to freeze the embedding matrix
freeze_embedd: true # This config is useless for model use bert embedding
drop_word:
  use_drop: false  # For DocRED, we do not use drop word, because of bert embedding
  drop: 0.5  # ratio of drop word
entity_type_size: 20
entity_type_num: 7
coref_size: 20
dis_size: 20

# encoder
nlayer: 2  # number of BiLSTM layer
cat_nlayer: false  # Whether cat output of BiLSTM layers
lstm_keep_prob: 0.8
hidden_size: 128

# attenders
entity_span_pooling: "mean"  #
coref_pooling: "mean"  # This is useless for EncAttAgg/EndAgg/BRAN-M model
which_model: "EncAgg"  # EncAttAgg, BiLSTM-M, BRAN-M
mutual_attender:
  attender: "NONE"  # ML-MMA(MultiLayer Mutual Multi-Head Attention), NONE
  num_layers: 1
  nhead: 8
  shared: true
  drop: 0.1
integration_attender:
  attender: "NONE"  # ML-MA(MultiLayer Multi-Head Attention), NONE
  num_layers: 1
  nhead: 8
  drop: 0.1
use_distance: true
use_overlap: true
use_bilinear: false  # This is useless for EndAgg model which always use Linear as final scorer
relation_num: 97  # 96 relation types plus `NONE`

# other hyper-parameters
batch_size: 4  # large batch_size will decrease the performance of EncAgg model
init_lr: 2.0e-4  # 2e-4
use_lr_scheduler: false  # we do not use lr_scheduler for all experiments
train_rel_limit_per_example: 200  # all positive pairs + random selected (200-#pos.) negative pairs, to train
max_epoch: 70
epoch_start_to_eval: 40  # evaluation takes the most time of training process, so during the first 40 epoch we do not eval
# For test
test_batch_size: 1  # aggregating all mention pairs is memory-consuming, so we set a small test batch size here
test_rel_limit_per_example: 1800  # all possible pairs should be fed for prediction
# filter out those entity pairs whose head-tail entity type combination does not appear in the pos pairs of training and dev sets.
filter_by_entity_type: false



