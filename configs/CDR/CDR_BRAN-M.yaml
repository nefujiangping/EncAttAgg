# data for train/test
data_path: "./data/CDR"
bert_embedding_dir: "./data/CDR"
checkpoint_dir: "./checkpoint"
log_dir: "./log"
summary_dir: "./summary"

dataset: "CDR"
train_prefix: "dev_train"  # train+dev
test_prefix: "dev_test"

# embedding
max_length: 620
use_bert_embedding: false
bert_embedd_dim: 768
# if use_bert_embedding=true and embedd_dim < bert_embedd_dim,
#  bert embedding will be down-project to bert_embedd_dim
embedd_dim: 200
# For `use_bert_embedding`=false, whether to freeze the embedding matrix
freeze_embedd: false
drop_word:
  use_drop: true  # For DocRED, we do not use drop word, because of bert embedding
  drop: 0.5  # ratio of drop word
entity_type_size: 20
entity_type_num: 3
coref_size: 20
dis_size: 20

# encoder
nlayer: 1  # number of BiLSTM layer
cat_nlayer: false  # Whether cat output of BiLSTM layers
lstm_keep_prob: 0.8
hidden_size: 128

# attenders
entity_span_pooling: "mean"  # average all tokens of a mention to get the mention representation
coref_pooling: "mean"  # average all mentions of an entity to get the entity representation
which_model: "BRAN-M"  # EncAgg, BiLSTM-M, BRAN-M
mutual_attender:
  attender: "NONE"  # ML-MMA(MultiLayer Mutual Multi-Head Attention), NONE
  num_layers: 1
  nhead: 8
  shared: true
  drop: 0.1
integration_attender:
  attender: "NONE"  # ML-MA(MultiLayer Multi-Head Attention), NONE
  num_layers: 1
  nhead: 8
  drop: 0.1
bi_affine_dropout: 0.5
use_distance: true
use_overlap: false
use_bilinear: true  # Using Bilinear performs better for BiLSTM-M model
relation_num: 2  # 96 relation types plus `NONE`
train_on_trainanddev: true  # merge train/dev sets to train model
lowercase: true  # whether lowercase tokens

# other hyper-parameters
batch_size: 16
init_lr: 3.0e-4  # 1e-3
use_lr_scheduler: false
train_rel_limit_per_example: 506  # 200 (the same setting as EncAttAgg model) performs worse
train_hypernym_filter: true
max_epoch: 25
epoch_start_to_eval: 0
# For test
test_batch_size: 1
test_rel_limit_per_example: 506
test_hypernym_filter: true
avg_params:
  use_avg_params: true
  num_recent_epochs: 10  # `0` means averaging the parameters up to this epoch



